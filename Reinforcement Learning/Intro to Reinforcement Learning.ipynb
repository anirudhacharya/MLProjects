{
  "metadata" : {
    "kernelspec" : {
      "display_name" : "Python 2",
      "language" : "python",
      "name" : "python2"
    },
    "language_info" : {
      "file_extension" : ".py",
      "mimetype" : "text/x-python",
      "name" : "python"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 2,
  "cells" : [ {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "Currently Covers - \n - MDPs\n - Policy Iteration\n - Value Iteration\n - Q-Learning" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "### Markov Decision Process" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import abc\n\nclass MDP:\n    __metaclass__ = abc.ABCMeta\n    def __init__(self, rewards, initialState, actionList):\n        self.initialState = initialState\n        self.rewards = rewards # the stateSpace can be got by rewars.keys()\n        self.actionList = actionList\n        pass\n    \n    @abc.abstractmethod\n    def getTransitionModel(self, state, action):\n        \"\"\"gives a transition model for a learned mdp model. Enter the state and \n        action. The output will be result state and the robability with which it \n        will reach the state\"\"\"\n        return\n    \n    def getRewards(self, state):\n        \"\"\"Given a state it will putput the reward for that state\"\"\"\n        return self.rewards[state]\n    \n    @abc.abstractmethod\n    def getPossibleActions(self, state):\n        \"\"\"Outputs the allowed actions in a particular state.\"\"\"\n        return" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "### Grid World Design" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import operator\n\nclass gridMDP(MDP):\n    def __init__(self, grid, initialState, terminals, gamma):\n        grid.reverse()\n        self.grid = grid\n        self.initialState = initialState\n        self.gamma = gamma\n        self.terminals = terminals\n        self.noOfRows= len(grid)\n        self.noOfCols = len(grid[0])\n        self.allStates = []\n        self.rewards = {}\n        for x in range(self.noOfCols):\n            for y in range(self.noOfRows):\n                if grid[y][x] is not None:\n                    self.allStates.append((x, y))\n                self.rewards[(x,y)] = grid[y][x]\n        self.actionList = [(1,0),(0,1),(-1,0),(0,-1)]\n        MDP.__init__(self, self.rewards, initialState, self.actionList)\n        #MDP, self).__init__(self.rewards, initialState, self.actionList)\n    \n    def getTransitionModel(self, state, action):\n        if state in self.terminals:\n            return [(state, 1.0)]\n        if action is None:\n            return [(state, 0.0)]\n        else:\n            return [(self.doAction(state, action), 0.8),\n                    (self.doAction(state, self.rightOf(action)), 0.1),\n                    (self.doAction(state, self.leftOf(action)), 0.1)]\n\n    def rightOf(self, action):\n        if action == (0,1):\n            return (1,0)\n        elif action==(1,0):\n            return (0,-1)\n        elif action==(0,-1):\n            return (-1,0)\n        elif action==(-1,0):\n            return (0,1)\n        else:\n            return action\n    \n    def leftOf(self, action):\n        if action == (0,1):\n            return (-1,0)\n        elif action==(1,0):\n            return (0,1)\n        elif action==(0,-1):\n            return (1,0)\n        elif action==(-1,0):\n            return (0,-1)\n        else:\n            return action\n    \n    def getPossibleActions(self, state):\n        return self.actionList\n    \n    def doAction(self, state, action):\n            \"Return the state that results from going in this direction.\"\n            resultState = tuple(map(operator.add, state, action))\n            if resultState in self.allStates:\n                return resultState\n            else:\n                return state \n            \n    def printGrid(self):\n        for i in reversed(self.grid):\n            print(\"\\t\".join(str(j) for j in i))\n    \n    def printUtilities(self, utilities):\n        utilityMat = [['a' for t in range(self.noOfCols)] for q in range(self.noOfRows)]\n        for i in range(self.noOfCols):\n            for j in range(self.noOfRows):\n                if (i,j) in utilities:\n                    utilityMat[j][i] = utilities[(i,j)]\n                else:\n                    utilityMat[j][i] = 'None'\n            utilityMat.reverse()\n        for i in range(len(utilityMat)):\n            print(utilityMat[i])\n            print('\\n')\n    \n    def printPolicy(self, policy):\n        for p in policy:\n            if policy[p] == (0,1):\n                policy[p] = '^'\n            elif policy[p] == (1,0):\n                policy[p] = '>'\n            elif policy[p] == (0,-1):\n                policy[p] = 'v'\n            elif policy[p] == (-1,0):\n                policy[p] = '<'\n            else:\n                policy[p] = '.'\n            policyMat = [['a' for t in range(self.noOfCols)] for q in range(self.noOfRows)]\n            for i in range(self.noOfCols):\n                for j in range(self.noOfRows):\n                    if (i,j) in policy:\n                        policyMat[j][i] = policy[(i,j)]\n                    else:\n                        policyMat[j][i] = '.'\n            policyMat.reverse()\n        for i in range(len(policyMat)):\n            print(policyMat[i])\n            print('\\n')" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "### Policy Iteration" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "def policyIteration(mdp):\n    U = {}\n    policy = {}\n    for s in mdp.allStates:\n        if s in mdp.terminals:\n            U[s] = mdp.rewards[s]\n            policy[s] = (0,0)\n        else:\n            U[s] = 0\n            policy[s] = (0,1)  #default policy is to go up in the grid\n    while True:\n        U = policyEvaluation(policy, U, mdp)\n        unchanged = True\n        for s in mdp.allStates:\n            if s not in mdp.terminals:\n                bestExpected = 0\n                preferredAction = (0,0)\n                for a in mdp.actionList:\n                    expectedUtility = 0\n                    for (s1,p) in mdp.getTransitionModel(s,a):\n                        expectedUtility = expectedUtility + (p*U[s1])\n                        if expectedUtility > bestExpected:\n                            bestExpected = expectedUtility\n                            preferredAction = a\n                if preferredAction != policy[s]:\n                    policy[s] = preferredAction    #We improve the policy for state s to action a\n                    unchanged = False\n        if unchanged:\n            return U, policy\n    return\n\ndef policyEvaluation(policy, U, mdp):\n    rewards, gamma = mdp.rewards, mdp.gamma\n    for s in mdp.allStates:\n        if s not in mdp.terminals:\n            U[s] = rewards[s]+(gamma * sum([p * U[s1]\n                                              for (s1, p)\n                                              in mdp.getTransitionModel(s, policy[s])]))\n    return U\n\ndef main():\n    grid43 = gridMDP([[-0.04, -0.04, -0.04, +1],\n                      [-0.04, None, -0.04, -1],\n                      [-0.04, -0.04, -0.04, -0.04]],\n                     (0,0),\n                     [(3, 2), (3, 1)],\n                     0.99)\n    grid33 = gridMDP([[-3, -1, 10],\n                      [-1, -1, -1],\n                      [-1, -1, -1]],\n                     (0,0),\n                     [(2,2)],\n                     0.99)\n    gridToRun = grid33\n    finalUtilities, optimalPolicy = policyIteration(gridToRun)\n    gridToRun.printUtilities(finalUtilities)\n    gridToRun.printGrid()\n    gridToRun.printPolicy(optimalPolicy)\n\nif __name__ == '__main__':\n    main()" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "### Modified Policy Iteration" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "def modifiedPolicyIteration(mdp):\n    U = {}\n    policy = {}\n    for s in mdp.allStates:\n        if s in mdp.terminals:\n            U[s] = mdp.rewards[s]\n            policy[s] = (0,0)\n        else:\n            U[s] = 0\n            policy[s] = (0,1)  #default policy is to go up in the grid\n    while True:\n        U = policyEvaluation(policy, U, mdp)\n        unchanged = True\n        for s in mdp.allStates:\n            if s not in mdp.terminals:\n                bestExpected = 0\n                preferredAction = (0,0)\n                for a in mdp.actionList:\n                    expectedUtility = 0\n                    for (s1,p) in mdp.getTransitionModel(s,a):\n                        expectedUtility = expectedUtility + (p*U[s1])\n                        if expectedUtility > bestExpected:\n                            bestExpected = expectedUtility\n                            preferredAction = a\n                if preferredAction != policy[s]:\n                    policy[s] = preferredAction    #We improve the policy for state s to action a\n                    unchanged = False\n        if unchanged:\n            return U, policy\n    return\n\ndef policyEvaluation(policy, U, mdp):\n    rewards, gamma = mdp.rewards, mdp.gamma\n    k = 20\n    for i in range(k):\n        for s in mdp.allStates:\n            if s not in mdp.terminals:\n                U[s] = rewards[s]+(gamma * sum([p * U[s1]\n                                                  for (s1, p)\n                                                  in mdp.getTransitionModel(s, policy[s])]))\n    return U\n\ndef main():\n    grid43 = gridMDP([[-0.04, -0.04, -0.04, +1],\n                      [-0.04, None, -0.04, -1],\n                      [-0.04, -0.04, -0.04, -0.04]],\n                     (0,0),\n                     [(3, 2), (3, 1)],\n                     0.99)\n    grid33 = gridMDP([[-3, -1, 10],\n                      [-1, -1, -1],\n                      [-1, -1, -1]],\n                     (0,0),\n                     [(2,2)],\n                     0.99)\n    gridToRun = grid43\n    finalUtilities, optimalPolicy = modifiedPolicyIteration(gridToRun)\n    gridToRun.printUtilities(finalUtilities)\n    gridToRun.printGrid()\n    gridToRun.printPolicy(optimalPolicy)\n\nif __name__ == '__main__':\n    main()" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "### Value Iteration" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "def valueIteration(mdp, epsilon):\n    U1 = {}\n    policy = {}\n    rewards, gamma = mdp.rewards, mdp.gamma\n    for s in mdp.allStates:\n        if s in mdp.terminals:\n            U1[s] = mdp.getRewards(s)\n            policy[s] = (0,0)\n        else:\n            U1[s] = 0\n            policy[s] = (0,0)\n    while True:\n        U = U1.copy()\n        delta = 0\n        for s in mdp.allStates:\n            if s not in mdp.terminals:\n                preferredAction = (0,0)\n                bestUtility = 0\n                for a in mdp.actionList:\n                    utility = 0\n                    for (s1,p) in mdp.getTransitionModel(s,a):\n                        utility = utility + (p*U[s1])\n                        if utility > bestUtility:\n                            bestUtility = utility\n                            preferredAction = a\n                U1[s] = rewards[s] + gamma * bestUtility\n                policy[s] = preferredAction\n                delta = max(delta, abs(U1[s] - U[s]))\n        if delta < epsilon * (1 - gamma) / gamma:\n            return U, policy\n\ndef main():\n    grid43 = gridMDP([[-0.04, -0.04, -0.04, +1],\n                      [-0.04, None, -0.04, -1],\n                      [-0.04, -0.04, -0.04, -0.04]],\n                     (0,0),\n                     [(3, 2), (3, 1)],\n                     0.99)\n    grid33 = gridMDP([[-3, -1, 10],\n                      [-1, -1, -1],\n                      [-1, -1, -1]],\n                     (0,0),\n                     [(2,2)],\n                     0.99)\n    gridToRun = grid43\n    finalUtilities, optimalPolicy = valueIteration(gridToRun, 0.1)\n    gridToRun.printUtilities(finalUtilities)\n    gridToRun.printGrid()\n    gridToRun.printPolicy(optimalPolicy)\n\nif __name__ == '__main__':\n    main()" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "### Q-Learning Agent" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import random\n\n'''\nInterpreting the different actions is as follows- \n(0,1)  --> Move Up\n(0,-1) --> Move Down\n(1,0)  --> Move Right\n(-1,0) --> Move Left\nInterpreting the states is straight forward and similar to what is done in the book AIMA.\n\n'''\nclass QLearningAgent(gridMDP):\n    def __init__(self, grid, initialState, terminals, gamma, alpha, N_e, epsilon, rewardPlus):\n        gridMDP.__init__(self, grid, initialState, terminals, gamma)\n        self.alpha = alpha\n        self.N_e = N_e\n        self.Q = {}\n        self.N = {}\n        self.epsilon = epsilon\n        self.rewardPlus = rewardPlus\n        for i in self.allStates:\n            for j in self.getPossibleActions(i):\n                self.Q[(i,j)] = 0\n                self.N[(i,j)] = 0\n    \n    #Overriding the transition Modle method, and returning junk values, becasue transition \n    #model should not be available to the learning agent.\n    def getTransitionModel(self, state, action):\n        return (state, 0)\n    \n    #Overriding the doAction method\n    def doAction(self, state, action):\n        stochasticProb = 0.8\n        rightAction = self.rightOf(action)\n        leftAction = self.leftOf(action)\n        randomNumber = random.random()\n        resultState = state\n        if randomNumber <= stochasticProb:\n            resultState = tuple(map(operator.add, state, action))\n        elif randomNumber > stochasticProb and randomNumber < (stochasticProb + \n                                                               ((1-stochasticProb)/2)):\n            resultState = tuple(map(operator.add, state, rightAction))\n        else:\n            resultState = tuple(map(operator.add, state, leftAction))\n        \n        if resultState not in self.allStates:\n            resultState = state\n        return resultState\n    \n    def getActionToPerform(self, curState):\n        dictActions = {}\n        for a in self.getPossibleActions(curState):\n            if self.N[(curState,a)] < self.N_e:\n                dictActions[self.rewardPlus] = a\n            else:\n                dictActions[self.Q[(curState,a)]] = a\n        return dictActions[max(dictActions)]\n    \n    def getAlternateActionToPerform(self, state):\n        dictActions = {}\n        for a in self.getPossibleActions(state):\n            dictActions[self.Q[(state,a)]] = a\n        \n        randomNumber = random.random()\n        if randomNumber < self.epsilon:\n            action = random.choice(self.getPossibleActions(state))\n            if self.N[(state,action)] > self.N_e:\n                return dictActions[max(dictActions)]\n            else:\n                return action\n        else:\n            return dictActions[max(dictActions)]\n    \n    #* self.N[(curState,action)] \n    def getUpdateValue(self, resultState, curState, action):\n        value = self.alpha * (self.getRewards(curState) +\n                              (self.gamma * self.getMaxQValue(resultState)) -\n                              self.Q[(curState, action)])\n        return value\n    \n    def makeUpdate(self, resultState, curState, actionTaken):\n        updateValue = self.getUpdateValue(resultState, curState, actionTaken)\n        self.Q[(curState,actionTaken)] += updateValue\n        return\n    \n    def getMaxQValue(self, state):\n        listOfQValues = []\n        for i in self.getPossibleActions(state):\n            listOfQValues.append(self.Q[(state, i)])\n        return max(listOfQValues)" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import matplotlib.pyplot as pyplot\n%matplotlib inline\ndef main():\n    qGrid43 = QLearningAgent([[-0.04, -0.04, -0.04, +1],\n                              [-0.04, None, -0.04, -1],\n                              [-0.04, -0.04, -0.04, -0.04]],\n                            (0,0),\n                            [(3, 2), (3, 1)],\n                            0.9,#gamma\n                            0.60, #alpha\n                            10,  #N_e\n                            0.6, #epsilon, greater the value of epsilon greater the random actions\n                            2) #rewardPlus\n    \n    #Another Trial Grid\n    qGrid33 = QLearningAgent([[-3, -1, 5],\n                              [-1, -1, -1],\n                              [-1, -1, -1]],\n                            (0,0),\n                            [(2,2)],\n                            0.9,#gamma\n                            0.10, #alpha\n                            30,  #N_e\n                            0.9, #epsilon, greater the value of epsilon greater the random actions\n                            2) #rewardPlus\n    \n    qGridToRun = qGrid43\n    \n    iterations = 500\n    x = []\n    y1 = []\n    y2 = []\n    y3 = []\n    for i in range(iterations):\n        curState = qGridToRun.initialState\n        while curState not in qGridToRun.terminals:\n            a = qGridToRun.getActionToPerform(curState)\n            #a = qGridToRun.getAlternateActionToPerform(curState)\n            resultState = qGridToRun.doAction(curState, a)\n            qGridToRun.N[(curState,a)] += 1\n            qGridToRun.makeUpdate(resultState, curState, a) \n            curState = resultState\n        qGridToRun.Q[(curState,a)] = qGridToRun.getRewards(curState)\n        x.append(i)\n        y1.append(qGridToRun.Q[(2,2),(1,0)])\n        y2.append(qGridToRun.Q[(0,0),(0,1)])\n        y3.append(qGridToRun.Q[(0,2),(1,0)])\n    \n    plotRMS(x,y1,'r')\n    plotRMS(x,y2,'b')\n    plotRMS(x,y3,'g')\n    pyplot.show()\n        \n    for s in qGridToRun.allStates:\n        if s not in qGridToRun.terminals:\n            print(s, qGridToRun.getActionToPerform(s))\n    \ndef plotRMS(x,y,c):\n    avg = sum(y)/len(y)\n    rmsV = []\n    for i in y:\n        rmsV.append(((i-avg) ** 2) ** 0.5)\n    pyplot.plot(x,rmsV,c)\n\nif __name__ == '__main__':\n    main()   " ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "import mxnet" ]
  } ]
}